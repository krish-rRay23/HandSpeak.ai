{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ–ï¸ HandSpeak.ai - Production Training Pipeline\n",
                "\n",
                "**Objective**: Train a Transformer-based ASL Character Recognition model.\n",
                "**Input**: 30-frame sequences of 21 MediaPipe hand landmarks (x,y,z).\n",
                "\n",
                "## Features\n",
                "- **Robust Preprocessing**: Wrist centralization, Scale Normalization, Rotation Alignment.\n",
                "- **Data Augmentation**: Simulates user variance (Rotation, Scale, Noise) and converts static frames to temporal sequences.\n",
                "- **Class Balancing**: Handles dataset imbalance using Weighted Loss.\n",
                "- **Visualization**: Verifies data quality and plots confusion matrices.\n",
                "- **Production Export**: Generates ONNX model for mobile deployment."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import math\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from glob import glob\n",
                "from collections import Counter\n",
                "\n",
                "# ----------------------------\n",
                "# CONFIGURATION\n",
                "# ----------------------------\n",
                "CONFIG = {\n",
                "    \"FRAMES\": 30,\n",
                "    \"LANDMARKS\": 21,\n",
                "    \"DIMS\": 3,\n",
                "    \"NUM_CLASSES\": 26,\n",
                "    \"BATCH_SIZE\": 64,\n",
                "    \"EPOCHS\": 50,\n",
                "    \"LR\": 1e-3,\n",
                "    \"SEED\": 42\n",
                "}\n",
                "\n",
                "def set_seed(seed):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "\n",
                "set_seed(CONFIG['SEED'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# UPDATE THIS PATH TO YOUR DATASET LOCATION\n",
                "DATASET_ROOT = '/content/drive/MyDrive/HandSpeak.ai/dataset_v2_mobile/landmarks'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing & Augmentation Engine\n",
                "Logic to normalize hands and simulate variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HandAugmentor:\n",
                "    \"\"\"Applies random rotation, scaling, and noise to landmarks.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def rotate_frame(frame, angle_deg):\n",
                "        rad = np.radians(angle_deg)\n",
                "        c, s = np.cos(rad), np.sin(rad)\n",
                "        # Rotate around Z-axis (2D plane of screen)\n",
                "        R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
                "        return np.dot(frame, R.T)\n",
                "\n",
                "    @staticmethod\n",
                "    def augment(sequence: np.ndarray) -> np.ndarray:\n",
                "        # sequence: (T, 21, 3)\n",
                "        \n",
                "        # 1. Random Rotation (-30 to +30 degrees)\n",
                "        angle = np.random.uniform(-30, 30)\n",
                "        \n",
                "        # 2. Random Scale (0.8x to 1.2x)\n",
                "        scale = np.random.uniform(0.8, 1.2)\n",
                "        \n",
                "        # 3. Random Shift/Translation (Global)\n",
                "        shift = np.random.uniform(-0.1, 0.1, size=(1, 3))\n",
                "        \n",
                "        processed = []\n",
                "        for frame in sequence:\n",
                "            f = HandAugmentor.rotate_frame(frame, angle)\n",
                "            f = f * scale\n",
                "            f = f + shift\n",
                "            processed.append(f)\n",
                "            \n",
                "        processed = np.array(processed)\n",
                "        \n",
                "        # 4. Pixel Noise (Independent per frame)\n",
                "        noise = np.random.normal(0, 0.005, processed.shape) # Small jitter\n",
                "        return processed + noise\n",
                "\n",
                "class Preprocessor:\n",
                "    \"\"\"Canonicalizes the hand: Centers Wrist, Normalizes Size, Aligns Rotation.\"\"\"\n",
                "    \n",
                "    def __init__(self, target_frames=30):\n",
                "        self.target_frames = target_frames\n",
                "\n",
                "    def process(self, raw_sequence: List[List[List[float]]]) -> np.ndarray:\n",
                "        data = np.array(raw_sequence, dtype=np.float32)\n",
                "        if data.ndim == 2: data = data[np.newaxis, ...]\n",
                "\n",
                "        # 1. Resample\n",
                "        data = self._resample(data)\n",
                "\n",
                "        # 2. Normalize\n",
                "        processed_frames = [self._normalize_frame(f) for f in data]\n",
                "        return np.stack(processed_frames)\n",
                "\n",
                "    def _resample(self, data):\n",
                "        T = data.shape[0]\n",
                "        if T == self.target_frames: return data\n",
                "        if T < self.target_frames:\n",
                "            # Pad with last frame\n",
                "            padding = np.tile(data[-1], (self.target_frames - T, 1, 1))\n",
                "            return np.concatenate([data, padding], axis=0)\n",
                "        else:\n",
                "            # Uniform sample\n",
                "            idx = np.linspace(0, T-1, self.target_frames, dtype=int)\n",
                "            return data[idx]\n",
                "\n",
                "    def _normalize_frame(self, frame):\n",
                "        WRIST, INDEX_MCP, MIDDLE_MCP = 0, 5, 9\n",
                "        \n",
                "        # Center\n",
                "        frame -= frame[WRIST]\n",
                "\n",
                "        # Scale\n",
                "        palm_size = np.linalg.norm(frame[MIDDLE_MCP])\n",
                "        if palm_size > 1e-6: frame /= palm_size\n",
                "\n",
                "        # Rotate Alignment (Align Wrist->Index to Y-axis)\n",
                "        v = frame[INDEX_MCP]\n",
                "        angle = np.arctan2(v[0], v[1])\n",
                "        theta = -angle\n",
                "        c, s = np.cos(theta), np.sin(theta)\n",
                "        R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
                "        frame = np.dot(frame, R.T)\n",
                "        \n",
                "        return frame"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CLASSES = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
                "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
                "IDX_TO_CLASS = {i: c for c, i in CLASS_TO_IDX.items()}\n",
                "\n",
                "def load_dataset(root_dir):\n",
                "    samples = []\n",
                "    labels = []\n",
                "    \n",
                "    pattern = os.path.join(root_dir, \"*_landmarks.json\")\n",
                "    files = glob(pattern)\n",
                "    print(f\"Searching {pattern}... Found {len(files)} files.\")\n",
                "    \n",
                "    for fpath in files:\n",
                "        try:\n",
                "            with open(fpath, 'r') as f:\n",
                "                data = json.load(f)\n",
                "                # Support both List[Dict] and Dict formats\n",
                "                if isinstance(data, dict): data = [data]\n",
                "                \n",
                "                for item in data:\n",
                "                    if 'landmarks' in item and len(item['landmarks']) == 21:\n",
                "                        samples.append(item['landmarks'])\n",
                "                        labels.append(CLASS_TO_IDX[item['letter']])\n",
                "        except Exception as e:\n",
                "            print(f\"Skipping {fpath}: {e}\")\n",
                "            \n",
                "    return samples, labels\n",
                "\n",
                "# Load Data\n",
                "try:\n",
                "    raw_samples, raw_labels = load_dataset(DATASET_ROOT)\n",
                "    print(f\"Total Samples: {len(raw_samples)}\")\n",
                "    \n",
                "    # Check Balance\n",
                "    counts = Counter([IDX_TO_CLASS[l] for l in raw_labels])\n",
                "    plt.figure(figsize=(12, 4))\n",
                "    plt.bar(counts.keys(), counts.values())\n",
                "    plt.title(\"Class Distribution\")\n",
                "    plt.show()\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Error loading data: {e}\")\n",
                "    raw_samples, raw_labels = [], []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_hand(landmarks, title=\"Hand\"):\n",
                "    \"\"\"2D Projection Plot\"\"\"\n",
                "    lms = np.array(landmarks)\n",
                "    plt.figure(figsize=(4,4))\n",
                "    plt.scatter(lms[:, 0], lms[:, 1], c='r')\n",
                "\n",
                "    # Connections (MediaPipe Hands)\n",
                "    CONNECTIONS = [\n",
                "        (0,1), (1,2), (2,3), (3,4), # Thumb\n",
                "        (0,5), (5,6), (6,7), (7,8), # Index\n",
                "        (0,9), (9,10), (10,11), (11,12), # Middle\n",
                "        (0,13), (13,14), (14,15), (15,16), # Ring\n",
                "        (0,17), (17,18), (18,19), (19,20) # Pinky\n",
                "    ]\n",
                "    \n",
                "    for start, end in CONNECTIONS:\n",
                "        plt.plot([lms[start, 0], lms[end, 0]], [lms[start, 1], lms[end, 1]], 'b-')\n",
                "\n",
                "    plt.gca().invert_yaxis() # Image coord system\n",
                "    plt.title(title)\n",
                "    plt.axis('equal')\n",
                "    plt.show()\n",
                "\n",
                "# Visualize a sample\n",
                "if raw_samples:\n",
                "    print(\"Raw Sample:\")\n",
                "    plot_hand(raw_samples[0], title=f\"Sample {IDX_TO_CLASS[raw_labels[0]]}\")\n",
                "    \n",
                "    # Visualize Preprocessed\n",
                "    prep = Preprocessor()\n",
                "    processed = prep.process([raw_samples[0]]) # Creates 30 frames\n",
                "    print(\"Preprocessed Frame 0:\")\n",
                "    plot_hand(processed[0], title=\"Normalized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset & Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ASLDataset(Dataset):\n",
                "    def __init__(self, samples, labels, augment=False):\n",
                "        self.samples = samples\n",
                "        self.labels = labels\n",
                "        self.augment = augment\n",
                "        self.preprocessor = Preprocessor(target_frames=30)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        raw = self.samples[idx]\n",
                "        \n",
                "        # Create Sequence Strategy\n",
                "        # 1. Start with static frame\n",
                "        base = np.array(raw, dtype=np.float32)\n",
                "        \n",
                "        # 2. Replicate to temporal window\n",
                "        sequence = np.tile(base[np.newaxis, ...], (30, 1, 1))\n",
                "        \n",
                "        # 3. Augment (Key step for training robust models on static data)\n",
                "        if self.augment:\n",
                "            sequence = HandAugmentor.augment(sequence)\n",
                "            \n",
                "        # 4. Canonicalize (Preprocess)\n",
                "        # Convert back to list for preprocessor compatibility if needed, or modify prep to take numpy\n",
                "        # My prep.process takes List[List...]. Let's just fix prep to handle numpy.\n",
                "        # Adapting here:\n",
                "        processed = self.preprocessor._resample(sequence)\n",
                "        processed = np.stack([self.preprocessor._normalize_frame(f) for f in processed])\n",
                "        \n",
                "        # Flatten: (30, 21, 3) -> (30, 63)\n",
                "        x = torch.tensor(processed, dtype=torch.float32).view(30, -1)\n",
                "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "        return x, y\n",
                "\n",
                "class HandTransformer(nn.Module):\n",
                "    def __init__(self, num_classes=26, d_model=64, nhead=4, num_layers=3, dim_feedforward=128):\n",
                "        super().__init__()\n",
                "        self.pos_encoder = nn.Parameter(torch.randn(1, 30, d_model) * 0.1)\n",
                "        self.input_proj = nn.Linear(63, d_model)\n",
                "        \n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model, \n",
                "            nhead=nhead, \n",
                "            dim_feedforward=dim_feedforward,\n",
                "            dropout=0.2,\n",
                "            batch_first=True # (Batch, Seq, Feat)\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
                "        \n",
                "        self.head = nn.Sequential(\n",
                "            nn.LayerNorm(d_model),\n",
                "            nn.Linear(d_model, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.2),\n",
                "            nn.Linear(64, num_classes)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: (B, 30, 63)\n",
                "        x = self.input_proj(x) + self.pos_encoder\n",
                "        x = self.transformer(x)\n",
                "        x = x.mean(dim=1) # Global Average Pooling\n",
                "        return self.head(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop with Class Balancing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X_train, X_val, y_train, y_val = train_test_split(\n",
                "    raw_samples, raw_labels, \n",
                "    test_size=0.2, \n",
                "    stratify=raw_labels, \n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "train_ds = ASLDataset(X_train, y_train, augment=True)\n",
                "val_ds = ASLDataset(X_val, y_val, augment=False)\n",
                "\n",
                "train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n",
                "\n",
                "# Compute Class Weights\n",
                "label_counts = Counter(y_train)\n",
                "total_samples = sum(label_counts.values())\n",
                "weights = []\n",
                "for i in range(CONFIG['NUM_CLASSES']):\n",
                "    if label_counts[i] > 0:\n",
                "        weights.append(total_samples / (CONFIG['NUM_CLASSES'] * label_counts[i]))\n",
                "    else:\n",
                "        weights.append(1.0)\n",
                "        \n",
                "class_weights = torch.FloatTensor(weights).cuda()\n",
                "print(f\"Class Weights: {class_weights[:5]}...\")\n",
                "\n",
                "# Init Model\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = HandTransformer(num_classes=CONFIG['NUM_CLASSES']).to(device)\n",
                "\n",
                "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['EPOCHS'])\n",
                "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
                "\n",
                "# Training Loop\n",
                "best_acc = 0\n",
                "history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "print(\"Starting Training...\")\n",
                "for epoch in range(CONFIG['EPOCHS']):\n",
                "    model.train()\n",
                "    train_loss, correct, total = 0, 0, 0\n",
                "    \n",
                "    for X, y in train_loader:\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        out = model(X)\n",
                "        loss = criterion(out, y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        _, pred = out.max(1)\n",
                "        correct += pred.eq(y).sum().item()\n",
                "        total += y.size(0)\n",
                "        \n",
                "    scheduler.step()\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    v_loss, v_correct, v_total = 0, 0, 0\n",
                "    with torch.no_grad():\n",
                "        for X, y in val_loader:\n",
                "            X, y = X.to(device), y.to(device)\n",
                "            out = model(X)\n",
                "            loss = criterion(out, y)\n",
                "            v_loss += loss.item()\n",
                "            _, pred = out.max(1)\n",
                "            v_correct += pred.eq(y).sum().item()\n",
                "            v_total += y.size(0)\n",
                "            \n",
                "    # Stats\n",
                "    t_acc = 100 * correct / total\n",
                "    v_acc = 100 * v_correct / v_total\n",
                "    history['loss'].append(train_loss / len(train_loader))\n",
                "    history['acc'].append(t_acc)\n",
                "    history['val_loss'].append(v_loss / len(val_loader))\n",
                "    history['val_acc'].append(v_acc)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1:02d} | Loss: {train_loss/len(train_loader):.4f} | Train Acc: {t_acc:.2f}% | Val Acc: {v_acc:.2f}%\")\n",
                "    \n",
                "    if v_acc > best_acc:\n",
                "        best_acc = v_acc\n",
                "        torch.save(model.state_dict(), \"best_model.pth\")\n",
                "\n",
                "print(f\"Done! Best Validation Acc: {best_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot History\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1,2,1)\n",
                "plt.plot(history['loss'], label='Train')\n",
                "plt.plot(history['val_loss'], label='Val')\n",
                "plt.title('Loss')\n",
                "plt.legend()\n",
                "plt.subplot(1,2,2)\n",
                "plt.plot(history['acc'], label='Train')\n",
                "plt.plot(history['val_acc'], label='Val')\n",
                "plt.title('Accuracy')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# Confusion Matrix\n",
                "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
                "model.eval()\n",
                "all_preds = []\n",
                "all_targets = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for X, y in val_loader:\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        out = model(X)\n",
                "        _, pred = out.max(1)\n",
                "        all_preds.extend(pred.cpu().numpy())\n",
                "        all_targets.extend(y.cpu().numpy())\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "cm = confusion_matrix(all_targets, all_preds)\n",
                "sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASSES, yticklabels=CLASSES, cmap='Blues')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('True')\n",
                "plt.show()\n",
                "\n",
                "print(classification_report(all_targets, all_preds, target_names=CLASSES))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export ONNX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dummy = torch.randn(1, 30, 63).to(device)\n",
                "torch.onnx.export(\n",
                "    model,\n",
                "    dummy,\n",
                "    \"asl_model_v2.onnx\",\n",
                "    input_names=['input'],\n",
                "    output_names=['output'],\n",
                "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}\n",
                ")\n",
                "print(\"Model exported as asl_model_v2.onnx\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}